{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc, ndcg_score\n",
    "from tdescore.classifier.train import train_classifier\n",
    "from tdescore.classifier.features import host_columns, early_columns, peak_columns, post_peak, parse_columns\n",
    "from tdescore.classifier.collate import get_classified_sources, convert_to_train_dataset\n",
    "import matplotlib.patheffects as path_effects\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shap\n",
    "from tdescore.classifier.collate import convert_to_train_dataset\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a370bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"tdescore\").setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [host_columns, early_columns, peak_columns, post_peak]\n",
    "labels = [\"Host Only\", \"Early\", \"At Peak\", \"Full\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns, column_descriptions = parse_columns(all_features[-1])\n",
    "full_columns, full_descriptions = parse_columns(post_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d1bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sources = get_classified_sources()\n",
    "data_to_use = convert_to_train_dataset(all_sources, columns=full_columns)\n",
    "nan_mask = np.array([np.sum(np.isnan(x)) > 0 for x in data_to_use])\n",
    "full_info_sources = all_sources[~nan_mask].reset_index(drop=True)\n",
    "full_info_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_use = full_info_sources\n",
    "df_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c130fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "n_estimators = [100.]\n",
    "\n",
    "param_res = dict()\n",
    "param_performance = []\n",
    "\n",
    "for i, features in enumerate(all_features):\n",
    "    relevant_columns, column_descriptions = parse_columns(features)\n",
    "\n",
    "    all_all_res, clfs = train_classifier(\n",
    "        train_sources=df_to_use,\n",
    "        n_iter=n_iter,\n",
    "        columns=relevant_columns,\n",
    "        n_estimator_set=n_estimators\n",
    "    )\n",
    "\n",
    "    n_estimator_set = list(sorted(clfs.keys()))\n",
    "    \n",
    "    metric = \"precision_recall_area\"\n",
    "\n",
    "    best_index = all_all_res[metric].idxmax()\n",
    "\n",
    "    best_estimator = all_all_res.iloc[best_index][\"n_estimator\"]\n",
    "\n",
    "    print(f\"Best value is {best_estimator}\")\n",
    "\n",
    "    clf = clfs[best_estimator]\n",
    "    all_res = all_all_res[all_all_res[\"n_estimator\"] == best_estimator][\"all_res\"].iloc[0]\n",
    "    \n",
    "    def flatten():\n",
    "        true_class = []\n",
    "        all_probs = []\n",
    "        for i in range(n_iter):\n",
    "            probs = all_res[f\"probs_{i}\"]\n",
    "            true_class += all_res[f\"class\"].tolist()\n",
    "            all_probs += probs.tolist()\n",
    "        return true_class, all_probs\n",
    "    \n",
    "    tclass, aprobs = flatten()\n",
    "    pr, recall, thresholds = metrics.precision_recall_curve(tclass, aprobs)\n",
    "    \n",
    "    roc_area = roc_auc_score(tclass, aprobs)\n",
    "    pr_area = auc(recall, pr)\n",
    "    \n",
    "    param_performance.append({\"Parameter Set\": labels[i], \"Total parameters\": len(relevant_columns), \"ROC area\": roc_area, \"Precision/Recall Area\": pr_area})\n",
    "    \n",
    "    param_res[i] = (pr, recall, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame(param_performance)\n",
    "param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "fscale = 4.\n",
    "figsize=(fscale*1.618, 2*fscale)\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212)\n",
    "\n",
    "keys = list(param_res.keys())[::-1]\n",
    "\n",
    "for i, key in enumerate(keys):\n",
    "    (pr, recall, thresholds) = param_res[key]\n",
    "    c = f\"C{i+1}\"\n",
    "    lab = labels[::-1][i]\n",
    "    ax1.plot(thresholds[:-1], pr[1:-1], color=c)\n",
    "    ax2.plot(thresholds[:-1], recall[1:-1], label=lab, color=c, linestyle=\"--\")\n",
    "\n",
    "for ax in [ax1, ax2]:    \n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(r\"$\\it{tdescore}$ threshold\")\n",
    "ax1.set_ylabel(\"Precision\")\n",
    "ax2.set_ylabel(\"Recall\")\n",
    "\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax1.set_yticklabels([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig(\"figures/precision_recall_comparison.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(1.5*fscale, 1.5*fscale)\n",
    "plt.figure(figsize=figsize)\n",
    "for i, key in enumerate(keys):\n",
    "    (pr, recall, thresholds) = param_res[key]\n",
    "    c = f\"C{i+1}\"\n",
    "    lab = labels[::-1][i]\n",
    "    plt.plot(pr, recall, label=lab, c=c)\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "# plt.legend()\n",
    "# plt.savefig(\"figures/precision_recall_comparison.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab444f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_str = r\"\"\"\\begin{table*}[]\n",
    "        \\begin{tabular}{c|c|c|c|c}\n",
    "        \\textbf{Parameter Set} & \\textbf{New Parameters} & \\textbf{Total Parameters} & \\textbf{ROC Area} & \\textbf{Precision/Recall Area} \\\\\n",
    "        \\hline\n",
    "\"\"\"\n",
    "print(text_str)\n",
    "\n",
    "used_parameters = []\n",
    "\n",
    "for i, row in param_df.iterrows():\n",
    "    relevant_columns, column_descriptions = parse_columns(all_features[i])\n",
    "    relevant_columns = [x for x in relevant_columns if x not in used_parameters]\n",
    "    name = relevant_columns[0].replace('_', '\\_')\n",
    "    print(\"\\t\" + r\" \\textbf{\" + row[0] + r\"}\" +f\" & {name} & {row[1]} & {row[2]:.2f} & {row[3]:.2f} \\\\\\\\\")\n",
    "    for x in relevant_columns[1:]:\n",
    "        name = x.replace('_', '\\_')\n",
    "        print(f\"\\t & {name} &  &  & \\\\\\\\\")\n",
    "        \n",
    "    used_parameters += relevant_columns\n",
    "        \n",
    "    print(\"\\t \\hline\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\caption{Performance of \\tdes for four parameter sets: information only about the host, information available shortly after discovery, information available by the time of peak, and the full parameter set. The performance of \\tdes substantially with more data, but high performance is only achieved for the full dataset.}\")\n",
    "print(r\"\"\"\\label{tab:parameter_subset}\n",
    "\\end{table*}\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0cb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame([relevant_columns, column_descriptions, list(clf.feature_importances_), ]).T\n",
    "features.sort_values(by=2, ascending=False, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6704c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdescore_env",
   "language": "python",
   "name": "tdescore_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
